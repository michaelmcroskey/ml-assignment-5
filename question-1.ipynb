{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 1\n",
    "Implement the delta training rule for a two-input linear unit. Train it to fit the target concept -2 + x1 + x2 > 0. You can (uniformly) randomly select x1 and x2 from -1000 to + 1000. Plot the error E as a function of the number of training iterations. Plot the decision surface after 5, 10, 50, 100, 500, … iterations.  (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Extra Credit: Try this using various constant values for η and using a decaying learning rate of η0 / i for the ith iteration. Which works better? (5 points)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### generate_data(x_min_range, x_max_range, N, m):\n",
    "Generates a data array used as the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_data(x_min_range, x_max_range, N, m):\n",
    "    data = np.random.uniform(x_min_range,x_max_range,size=[N,m])\n",
    "    data = np.concatenate((np.ones([N,1]),data),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### gradient_descent(training_examples, n)\n",
    "Each training example is a pair of the form <x, t> where x is the vector of input values and t is the target output value.  n is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(training_examples, n):\n",
    "    # Training iterations\n",
    "    iterations = [5, 10, 50, 100, 500]\n",
    "    \n",
    "    # Initialize iter_error\n",
    "    iter_error = []\n",
    "    \n",
    "    # Initialize each w_i to some random small value\n",
    "    w = np.random.random((2))/100\n",
    "    \n",
    "    # Initialize termination condition\n",
    "    counter = 1\n",
    "    \n",
    "    # Initialize delta_w to 0\n",
    "    delta_w = [0, 0]\n",
    "\n",
    "    # Until termination condition is met\n",
    "    while counter <= 500:\n",
    "        # Initialize E\n",
    "        E = 0.0\n",
    "        \n",
    "        # Loop through the training examples\n",
    "        for t in training_examples:\n",
    "            # Input the instance to the unit and compute the output o\n",
    "            unit_calculation = t[1] + t[2] + -2\n",
    "            target = 1\n",
    "            if unit_calculation < 0:\n",
    "                target = -1\n",
    "\n",
    "            # Define output\n",
    "            output = direction(np.dot([t[1],t[2]],w))\n",
    "            \n",
    "            # Calculate delta_w_i\n",
    "            for j in range(len(w)):\n",
    "                delta_w[j] = delta_w[j] + n*(target - output)*t[j+1]\n",
    "                 \n",
    "            E += (target-output)*(target-output)*.5\n",
    "\n",
    "   \n",
    "        # Calculate w_i\n",
    "        for k in range(len(w)):\n",
    "            w[k] = delta_w[k] + w[k]\n",
    "        \n",
    "        \n",
    "        # Update the termination counter\n",
    "        counter += 1\n",
    "\n",
    "        # Print at specific points\n",
    "        if (counter == 5) or (counter == 10) or (counter == 50) or (counter == 100) or (counter == 500):\n",
    "            print \"Reached the end for iteration #\", counter\n",
    "            print \"E = \", E\n",
    "            print \"-----\"\n",
    "        \n",
    "        # Append it to the iteration error\n",
    "        iter_error.append(E)\n",
    "        \n",
    "    # Decision Surface\n",
    "    decision_surface = []\n",
    "    for t in training_examples:\n",
    "        decision_surface.append(sum([i * j for i,j in zip(w, t[1:3])]) > 0)\n",
    "        \n",
    "    plt.scatter([x[1] for x in training_examples], [x[2] for x in training_examples], c=decision_surface)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "        \n",
    "    # Plot the results\n",
    "    plt.plot([x for x in range(0,500)], [y for y in iter_error])\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Sum of Squared Error')\n",
    "    plt.show()\n",
    "\n",
    "def direction(val):\n",
    "    if val>0:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient_descent_ec(training_examples, n)\n",
    "Extra credit implementation that is called in a loop for varying values of n in the main program calls below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_ec(training_examples):\n",
    "    n_array = [.01, .02, .03, .04, .05]\n",
    "    iter_error_overall = []\n",
    "    \n",
    "    for n_o in n_array:\n",
    "        print \"**************************************\"\n",
    "        print \"Performing gradient descent for \", n_o\n",
    "        \n",
    "        # Training iterations\n",
    "        iterations = [5, 10, 50, 100, 500]\n",
    "\n",
    "        # Initialize iter_error\n",
    "        iter_error = []\n",
    "\n",
    "        # Initialize each w_i to some random small value\n",
    "        w = np.random.random((2))/100\n",
    "\n",
    "        # Initialize termination condition\n",
    "        counter = 1\n",
    "\n",
    "        # Initialize delta_w to 0\n",
    "        delta_w = [0, 0]\n",
    "\n",
    "        # Until termination condition is met\n",
    "        while counter <= 500:\n",
    "            # Initialize E\n",
    "            E = 0.0\n",
    "\n",
    "            # Loop through the training examples\n",
    "            for t in training_examples:\n",
    "                # Input the instance to the unit and compute the output o\n",
    "                unit_calculation = t[1] + t[2] + -2\n",
    "                target = 1\n",
    "                if unit_calculation < 0:\n",
    "                    target = -1\n",
    "\n",
    "                # Define output\n",
    "                output = direction(np.dot([t[1],t[2]],w))\n",
    "\n",
    "                # DECAYING LEARNING RATE\n",
    "                n = float(n_o) / float(counter)\n",
    "\n",
    "                # Calculate delta_w_i\n",
    "                for j in range(len(w)):\n",
    "                    delta_w[j] = delta_w[j] + n*(target - output)*t[j+1]\n",
    "\n",
    "                E += (target-output)*(target-output)*.5\n",
    "\n",
    "\n",
    "            # Calculate w_i\n",
    "            for k in range(len(w)):\n",
    "                w[k] = delta_w[k] + w[k]\n",
    "\n",
    "\n",
    "            # Update the termination counter\n",
    "            counter += 1\n",
    "\n",
    "            # Print at specific points\n",
    "            if (counter == 5) or (counter == 10) or (counter == 50) or (counter == 100) or (counter == 500):\n",
    "                print \"Reached the end for iteration #\", counter\n",
    "                print \"E = \", E\n",
    "                print \"-----\"\n",
    "\n",
    "            # Append it to the iteration error\n",
    "            iter_error.append(E)\n",
    "            \n",
    "        iter_error_overall.append(iter_error)\n",
    "        \n",
    "    # Plot the results\n",
    "    l0, = plt.plot([x for x in range(0,500)], [y for y in iter_error_overall[0]], label=str(n_o))\n",
    "    l1, = plt.plot([x for x in range(0,500)], [y for y in iter_error_overall[1]], label=str(n_o))\n",
    "    l2, = plt.plot([x for x in range(0,500)], [y for y in iter_error_overall[2]], label=str(n_o))\n",
    "    l3, = plt.plot([x for x in range(0,500)], [y for y in iter_error_overall[3]], label=str(n_o))\n",
    "    l4, = plt.plot([x for x in range(0,500)], [y for y in iter_error_overall[4]], label=str(n_o))\n",
    "    \n",
    "    plt.legend((l0, l1, l2, l3, l4), ('.01', '.02', '.03', '.04', '.05'), loc='upper right', shadow=True)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Sum of Squared Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main function calls\n",
    "Below code calls functions necessary to implement delta rule for function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end for iteration # 5\n",
      "E =  1054.0\n",
      "-----\n",
      "Reached the end for iteration # 10\n",
      "E =  406.0\n",
      "-----\n",
      "Reached the end for iteration # 50\n",
      "E =  108.0\n",
      "-----\n",
      "Reached the end for iteration # 100\n",
      "E =  26.0\n",
      "-----\n",
      "Reached the end for iteration # 500\n",
      "E =  22.0\n",
      "-----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c9d6c00f6d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgradient_descent_ec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a995bd1f25ba>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(training_examples, n)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdecision_surface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdecision_surface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecision_surface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "data = generate_data(-1000, 1000, 5000, 2)\n",
    "n = 0.05\n",
    "gradient_descent(data, n)\n",
    "gradient_descent_ec(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
